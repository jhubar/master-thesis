{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhubar/master-thesis/blob/main/clupoints_exo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbNx47L-cb8H",
        "outputId": "8df5a7e8-1c98-4ce9-d802-26896b6b6b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp77yMnb7_le",
        "outputId": "06f6cc03-821a-4937-859a-fbed0aaadafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 100003, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 100003 (delta 119), reused 143 (delta 75), pack-reused 99753\u001b[K\n",
            "Receiving objects: 100% (100003/100003), 93.67 MiB | 14.23 MiB/s, done.\n",
            "Resolving deltas: 100% (73676/73676), done.\n",
            "/bin/bash: line 0: cd: tranformers: No such file or directory\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!rm -r transformers\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!cd tranformers\n",
        "!pip install -q ./transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNFM2pA1l7It"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import CamembertTokenizer\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "fGMKFBnBeNPr",
        "outputId": "ac5e10ae-e4c2-41a1-9eaf-c6d0a1a3d08b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f1aea2b5-6e50-4c9a-930f-344ef9091e1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1aea2b5-6e50-4c9a-930f-344ef9091e1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1aea2b5-6e50-4c9a-930f-344ef9091e1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1aea2b5-6e50-4c9a-930f-344ef9091e1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3  3630_4          0  It must be assumed that those who praised this...\n",
              "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/movie_review_train.tsv\", sep='\\t')\n",
        "df.head() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Gxycfno87qjy",
        "outputId": "bfe49694-cfd0-4443-dab5-12b955127fac"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "df['sentiment'].value_counts().plot.barh()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "ufu0jCdL8tDv",
        "outputId": "d2a76653-f159-499c-a5f2-70ec47a25694"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df[\"words_per_review\"] = df[\"review\"].str.split().apply(len)\n",
        "df.boxplot(\"words_per_review\", by = 'sentiment',grid = False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iuv4YnCk9jNN"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "class TrainSetBuilder():\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "     # Hyper parameters\n",
        "      self.max_len = 500      # Maximum number of words in a sequence\n",
        "      self.train_split = 0.8  # Propotion of the dataset for training set\n",
        "      self.batch_size = 10    # Number of elements in a batch\n",
        "\n",
        "      # Training dataset\n",
        "      self.train_dataset = None\n",
        "      # Testing dataset\n",
        "      self.test_dataset = None\n",
        "      # Classes\n",
        "      self.class_labels = ['Negative', 'Positive']\n",
        "\n",
        "      # Data handlers\n",
        "      self.train_handler = None\n",
        "      self.test_handler = None\n",
        "      self.df = None\n",
        "\n",
        "      # Load the tokenizer\n",
        "      self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "  def import_movie_review(self, path='/content/drive/MyDrive/movie_review_train.tsv',\n",
        "                             reduce=None):\n",
        "        \n",
        "        self.df = pd.read_csv(path, sep='\\t')\n",
        "        # Import data\n",
        "        \n",
        "        # Concat data\n",
        "        reviews = np.array(df['review'])\n",
        "        sentiments = np.array(df['sentiment'])\n",
        "        \n",
        "        # To list\n",
        "        reviews = reviews.tolist()\n",
        "\n",
        "        # If reduce (doesn't load all the dataset\n",
        "        if reduce is not None:\n",
        "            print('WARNING: reduced dataset: for deboging purposes only')\n",
        "            reviews = reviews[0:reduce]\n",
        "            sentiments = sentiments[0:reduce]\n",
        "\n",
        "        # Encode the batch of data\n",
        "        print('Data tokenization...')\n",
        "        encoded_batch = self.tokenizer.batch_encode_plus(reviews,\n",
        "                                                         add_special_tokens=True,\n",
        "                                                         max_length=self.max_len,\n",
        "                                                         padding=True,\n",
        "                                                         truncation=True,\n",
        "                                                         return_attention_mask=True,\n",
        "                                                         return_tensors='pt')\n",
        "        print('... Done')\n",
        "        # Get the spliting index\n",
        "        split_border = int(len(sentiments)*self.train_split)\n",
        "        # Get a tensor for sentiments\n",
        "        sentiments = torch.tensor(sentiments)\n",
        "        # Now encode datasets tensors\n",
        "        print('Tensors encoding...')\n",
        "        self.train_dataset = TensorDataset(\n",
        "            encoded_batch['input_ids'][:split_border],\n",
        "            encoded_batch['attention_mask'][:split_border],\n",
        "            sentiments[:split_border])\n",
        "        self.test_dataset = TensorDataset(\n",
        "            encoded_batch['input_ids'][split_border:],\n",
        "            encoded_batch['attention_mask'][split_border:],\n",
        "            sentiments[split_border:])\n",
        "        print('... Done')\n",
        "\n",
        "        # Get data handler\n",
        "        print('Data handler encoding...')\n",
        "        self.train_handler = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler=RandomSampler(self.train_dataset),\n",
        "            batch_size=self.batch_size)\n",
        "\n",
        "        self.test_handler = DataLoader(\n",
        "            self.test_dataset,\n",
        "            sampler=SequentialSampler(self.test_dataset),\n",
        "            batch_size=self.batch_size)\n",
        "        print('... Done')\n",
        "        print('End of dataset encoding.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoTtHqXLi4sc",
        "outputId": "68386977-d867-41c1-b02b-46acde6f4660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data tokenization...\n",
            "... Done\n",
            "Tensors encoding...\n",
            "... Done\n",
            "Data handler encoding...\n",
            "... Done\n",
            "End of dataset encoding.\n"
          ]
        }
      ],
      "source": [
        "# Get the dataset instance\n",
        "dataset = TrainSetBuilder()\n",
        "# Import allo_cine data\n",
        "dataset.import_movie_review(reduce=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jP38stu2Gipt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from math import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "class distilBERT():\n",
        "\n",
        "    def __init__(self, name='Final_Work',\n",
        "                 colab=False,  # If we are using google colab\n",
        "                 reset_if_exist=False,  # If we want to delete an existing model to train a new\n",
        "                 force_cpu=False,  # Force cpu using while cuda is present\n",
        "                 externe=False,\n",
        "                 ):\n",
        "\n",
        "        # Hyper parameters\n",
        "        self.learning_rate = 5e-7  # Learning rate for Adam optimizer\n",
        "        self.epslilon = 1e-8  # Epsilon limit for optimizer\n",
        "        self.pred_batch_size = 1000  # Batch size for predictions\n",
        "\n",
        "        # Download pre-trained camembert\n",
        "        self.model = DistilBertForSequenceClassification.from_pretrained(\n",
        "            'distilbert-base-uncased',\n",
        "            num_labels=2  # Binary classification\n",
        "        )\n",
        "        # Classes labels\n",
        "        self.class_labels = ['Negative', 'Positive']\n",
        "\n",
        "        # The optimizer to use\n",
        "        self.optimizer = AdamW(self.model.parameters(),\n",
        "                               lr=self.learning_rate,\n",
        "                               eps=self.epslilon)\n",
        "\n",
        "        # Get the tokenizer from BERT pre-trained\n",
        "        self.tokenizer =  DistilBertTokenizer.from_pretrained(\n",
        "            'distilbert-base-uncased'\n",
        "           \n",
        "        )\n",
        "        # A softmax function for predictions\n",
        "        self.soft_pred = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        # Device:\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # The actual epoch idx and total tracking index\n",
        "        self.epoch_idx = 0\n",
        "        self.total_idx = 0\n",
        "\n",
        "        # Some paths and name:\n",
        "        self.name = name\n",
        "        tmp = ''\n",
        "        \n",
        "        tmp = '/content/drive/MyDrive/cluepoints/'\n",
        "        \n",
        "        self.model_path = '{}Model/{}/Weights'.format(tmp, name)\n",
        "        self.optimizer_path = '{}Model/{}/Optimizer'.format(tmp, name)\n",
        "        self.tracking_path = '{}Model/{}/Tracking'.format(tmp, name)\n",
        "\n",
        "        # # Load model if exist\n",
        "        # gen_idx = 0\n",
        "        # mod_lst = os.listdir(self.model_path)\n",
        "        # if len(mod_lst) > 0:\n",
        "        #     idx_lst = []\n",
        "        #     for itm in mod_lst:\n",
        "        #         idx_lst.append(int(itm.replace('.pt', '')))\n",
        "        #     gen_idx = int(max(idx_lst))\n",
        "\n",
        "        # try:\n",
        "        #     self.model.load_state_dict(torch.load('{}/{}.pt'.format(self.model_path, gen_idx)))\n",
        "        #     print('Existing model successfully loaded')\n",
        "        #     self.optimizer.load_state_dict(torch.load('{}/{}'.format(self.optimizer_path, gen_idx)))\n",
        "        #     print('Optimizer state successfully loaded')\n",
        "        # except:\n",
        "        #     print('WARNING: No Model to restore')\n",
        "        #     pass\n",
        "        # gen_idx += 1\n",
        "\n",
        "        # # Get last epoch idx\n",
        "        # try:\n",
        "        #     df = pd.read_csv('{}/Total_Tracking.csv'.format(self.tracking_path), sep=';', header=None).to_numpy()\n",
        "        #     self.epoch_idx = int(np.max(df[:, 0])) + 1\n",
        "        # except:\n",
        "        #     pass\n",
        "\n",
        "        # self.total_idx = gen_idx\n",
        "\n",
        "\n",
        "    def train(self,\n",
        "              dataset,  # A TrainSetBuilder element\n",
        "              nb_epoch=100  # The number of epochs to perform\n",
        "              ):\n",
        "\n",
        "        # Get data handlers\n",
        "        train_dataloader = dataset.train_handler\n",
        "        test_dataloader = dataset.test_handler\n",
        "\n",
        "        # Get an iterator for the testing loader\n",
        "        test_iter = iter(test_dataloader)\n",
        "\n",
        "        # Get epochs index\n",
        "        e_start = self.epoch_idx\n",
        "        e_end = self.epoch_idx + nb_epoch\n",
        "\n",
        "        # The training loop\n",
        "        for epoch in range(e_start, e_end):\n",
        "            tmp_train_loss = []\n",
        "            # Training mode\n",
        "            self.model.train()\n",
        "            epoch_train_loss = []\n",
        "            epoch_test_loss = []\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                # Delete cache\n",
        "                torch.cuda.empty_cache()\n",
        "                # Set batch's data to gpu\n",
        "                input_id = batch[0].to(self.device)\n",
        "                attention_mask = batch[1].to(self.device)\n",
        "                sentiment = batch[2].to(self.device)\n",
        "                # Swipe gradients\n",
        "                self.model.zero_grad()\n",
        "                # Predict\n",
        "                prd = self.model(input_id,\n",
        "                                 \n",
        "                                 attention_mask=attention_mask,\n",
        "                                 labels=sentiment)\n",
        "                loss = prd.loss\n",
        "                logits = prd.logits\n",
        "                # Backward and optimization\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                # Store temp loss\n",
        "                tmp_train_loss.append(loss.cpu().detach().item())\n",
        "                epoch_train_loss.append(loss.cpu().detach().item())\n",
        "\n",
        "                # Tracking each 50 batchs\n",
        "                if step % 50 == 0:\n",
        "                    tmp_test_loss = []\n",
        "                    # Compute 10 batchs on the test set\n",
        "                    self.model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        for t in range(0, 10):\n",
        "                            try:\n",
        "                                test_batch = next(test_iter)\n",
        "                            except:\n",
        "                                test_iter = iter(test_dataloader)\n",
        "                                test_batch = next(test_iter)\n",
        "                            # Get data\n",
        "                            input_id = test_batch[0].to(self.device)\n",
        "                            attention_mask = test_batch[1].to(self.device)\n",
        "                            sentiment = test_batch[2].to(self.device)\n",
        "                            # Predict\n",
        "                            prd = self.model(input_id,\n",
        "                                             attention_mask=attention_mask,\n",
        "                                             labels=sentiment)\n",
        "                            test_loss = prd.loss\n",
        "                            tmp_test_loss.append(test_loss.cpu().detach().item())\n",
        "                            epoch_test_loss.append(test_loss.cpu().detach().item())\n",
        "\n",
        "                    # Get the mean loss\n",
        "                    train_loss = np.mean(tmp_train_loss)\n",
        "                    tmp_train_loss = []\n",
        "                    test_loss = np.mean(tmp_test_loss)\n",
        "\n",
        "                    print('Epoch {}, step {} - Total_idx {} - Train Loss: {} - Test Loss: {}'.format(epoch,\n",
        "                                                                                        step,\n",
        "                                                                                        self.total_idx,\n",
        "                                                                                        train_loss,\n",
        "                                                                                        test_loss))\n",
        "                    # Write in file\n",
        "                    tmp = [str(epoch), str(self.total_idx), str(step), str(train_loss), str(test_loss)]\n",
        "                    file = open('{}/Total_Tracking.csv'.format(self.tracking_path), 'a')\n",
        "                    file.write('{}\\n'.format(';'.join(tmp)))\n",
        "                    file.close()\n",
        "\n",
        "                    # Save parameters\n",
        "                    torch.save(self.model.state_dict(), '{}/{}.pt'.format(self.model_path, self.total_idx))\n",
        "                    torch.save(self.optimizer.state_dict(), '{}/{}.pt'.format(self.optimizer_path, self.total_idx))\n",
        "                    # Update general idx\n",
        "                    self.total_idx += 1\n",
        "\n",
        "                    # # Delete old models\n",
        "                    # mod_lst = os.listdir(self.model_path)\n",
        "                    # for itm in mod_lst:\n",
        "                    #     itm_idx = int(itm.replace('.pt', ''))\n",
        "                    #     if itm_idx < self.total_idx and self.total_idx - itm_idx > 5:\n",
        "                    #         if itm_idx % 5 != 0:\n",
        "                    #             try:\n",
        "                    #                 os.remove('{}/{}.pt'.format(self.model_path, itm_idx))\n",
        "                    #                 os.remove('{}/{}.pt'.format(self.optimizer_path, itm_idx))\n",
        "                    #             except:\n",
        "                    #                 print('Error during old model deleting')\n",
        "\n",
        "            # Epoch loss computing\n",
        "            ep_train_loss = np.mean(epoch_train_loss)\n",
        "            ep_test_loss = np.mean(epoch_test_loss)\n",
        "\n",
        "            file = open('{}/Epoch_Tracking.csv'.format(self.tracking_path), 'a')\n",
        "            file.write('{};{};{}\\n'.format(epoch, ep_train_loss, ep_test_loss))\n",
        "            file.close()\n",
        "            print('\\t* ===================================== *')\n",
        "            print('\\t* Epoch {} :'.format(epoch))\n",
        "            print('\\t* \\t Average Train Loss: {}'.format(ep_train_loss))\n",
        "            print('\\t* \\t Average Test Loss: {}'.format(ep_test_loss))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qyp7M6ppGvNY",
        "outputId": "db83f912-fbea-4aa5-dc39-cc2778bbe195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, step 0 - Total_idx 0 - Train Loss: 0.7170134782791138 - Test Loss: 0.6983825206756592\n",
            "Epoch 0, step 50 - Total_idx 1 - Train Loss: 0.6932957899570465 - Test Loss: 0.6934653162956238\n",
            "Epoch 0, step 100 - Total_idx 2 - Train Loss: 0.6941992402076721 - Test Loss: 0.6909090340137481\n",
            "Epoch 0, step 150 - Total_idx 3 - Train Loss: 0.6900856018066406 - Test Loss: 0.6850231170654297\n",
            "Epoch 0, step 200 - Total_idx 4 - Train Loss: 0.6899113142490387 - Test Loss: 0.6836056053638458\n",
            "Epoch 0, step 250 - Total_idx 5 - Train Loss: 0.6855999827384949 - Test Loss: 0.6822867870330811\n",
            "Epoch 0, step 300 - Total_idx 6 - Train Loss: 0.6821113753318787 - Test Loss: 0.6828506588935852\n",
            "Epoch 0, step 350 - Total_idx 7 - Train Loss: 0.6799024450778961 - Test Loss: 0.6769521653652191\n",
            "Epoch 0, step 400 - Total_idx 8 - Train Loss: 0.6737342774868011 - Test Loss: 0.6708295106887817\n",
            "Epoch 0, step 450 - Total_idx 9 - Train Loss: 0.6655327010154725 - Test Loss: 0.6598804116249084\n",
            "Epoch 0, step 500 - Total_idx 10 - Train Loss: 0.65193235039711 - Test Loss: 0.6441152155399322\n",
            "Epoch 0, step 550 - Total_idx 11 - Train Loss: 0.6358928596973419 - Test Loss: 0.6284196436405182\n",
            "Epoch 0, step 600 - Total_idx 12 - Train Loss: 0.6096462070941925 - Test Loss: 0.5976001441478729\n",
            "Epoch 0, step 650 - Total_idx 13 - Train Loss: 0.5655440777540207 - Test Loss: 0.5259968191385269\n",
            "Epoch 0, step 700 - Total_idx 14 - Train Loss: 0.5124296617507934 - Test Loss: 0.5018677204847336\n",
            "Epoch 0, step 750 - Total_idx 15 - Train Loss: 0.48543121218681334 - Test Loss: 0.4616629481315613\n",
            "Epoch 0, step 800 - Total_idx 16 - Train Loss: 0.44700030744075775 - Test Loss: 0.39943758845329286\n",
            "Epoch 0, step 850 - Total_idx 17 - Train Loss: 0.4131425589323044 - Test Loss: 0.43485404551029205\n",
            "Epoch 0, step 900 - Total_idx 18 - Train Loss: 0.3838269540667534 - Test Loss: 0.3060737743973732\n",
            "Epoch 0, step 950 - Total_idx 19 - Train Loss: 0.3579131066799164 - Test Loss: 0.36104286164045335\n",
            "Epoch 0, step 1000 - Total_idx 20 - Train Loss: 0.38523576766252515 - Test Loss: 0.3421216905117035\n",
            "Epoch 0, step 1050 - Total_idx 21 - Train Loss: 0.3412109187245369 - Test Loss: 0.31296984404325484\n",
            "Epoch 0, step 1100 - Total_idx 22 - Train Loss: 0.31336712181568144 - Test Loss: 0.366702239215374\n",
            "Epoch 0, step 1150 - Total_idx 23 - Train Loss: 0.320050473511219 - Test Loss: 0.3585648611187935\n",
            "Epoch 0, step 1200 - Total_idx 24 - Train Loss: 0.32389296144247054 - Test Loss: 0.4106608971953392\n",
            "Epoch 0, step 1250 - Total_idx 25 - Train Loss: 0.3126497918367386 - Test Loss: 0.3073701187968254\n",
            "Epoch 0, step 1300 - Total_idx 26 - Train Loss: 0.2840880092978477 - Test Loss: 0.4038642719388008\n",
            "Epoch 0, step 1350 - Total_idx 27 - Train Loss: 0.2848774439096451 - Test Loss: 0.3114826545119286\n",
            "Epoch 0, step 1400 - Total_idx 28 - Train Loss: 0.31481308564543725 - Test Loss: 0.25636234134435654\n",
            "Epoch 0, step 1450 - Total_idx 29 - Train Loss: 0.2726054485142231 - Test Loss: 0.4072313651442528\n",
            "Epoch 0, step 1500 - Total_idx 30 - Train Loss: 0.291419058740139 - Test Loss: 0.32539189532399176\n",
            "Epoch 0, step 1550 - Total_idx 31 - Train Loss: 0.33053350672125814 - Test Loss: 0.2331875428557396\n",
            "Epoch 0, step 1600 - Total_idx 32 - Train Loss: 0.26122762471437455 - Test Loss: 0.35093145817518234\n",
            "Epoch 0, step 1650 - Total_idx 33 - Train Loss: 0.3285263234376907 - Test Loss: 0.3169065319001675\n",
            "Epoch 0, step 1700 - Total_idx 34 - Train Loss: 0.2597212319076061 - Test Loss: 0.2731862038373947\n",
            "Epoch 0, step 1750 - Total_idx 35 - Train Loss: 0.2680360953509808 - Test Loss: 0.3402692496776581\n",
            "Epoch 0, step 1800 - Total_idx 36 - Train Loss: 0.3404998251795769 - Test Loss: 0.2827294573187828\n",
            "Epoch 0, step 1850 - Total_idx 37 - Train Loss: 0.2758434620499611 - Test Loss: 0.30254477858543394\n",
            "Epoch 0, step 1900 - Total_idx 38 - Train Loss: 0.2728541542589664 - Test Loss: 0.23912040889263153\n",
            "Epoch 0, step 1950 - Total_idx 39 - Train Loss: 0.2768396830558777 - Test Loss: 0.2368616931140423\n",
            "\t* ===================================== *\n",
            "\t* Epoch 0 :\n",
            "\t* \t Average Train Loss: 0.43851137670874596\n",
            "\t* \t Average Test Loss: 0.4515928847901523\n",
            "Epoch 1, step 0 - Total_idx 40 - Train Loss: 0.2173011749982834 - Test Loss: 0.25395395010709765\n",
            "Epoch 1, step 50 - Total_idx 41 - Train Loss: 0.2821270999312401 - Test Loss: 0.23260803446173667\n",
            "Epoch 1, step 100 - Total_idx 42 - Train Loss: 0.2829103596508503 - Test Loss: 0.32797817662358286\n",
            "Epoch 1, step 150 - Total_idx 43 - Train Loss: 0.2701447190344334 - Test Loss: 0.28037568628787995\n",
            "Epoch 1, step 200 - Total_idx 44 - Train Loss: 0.24698960065841674 - Test Loss: 0.2650796137750149\n",
            "Epoch 1, step 250 - Total_idx 45 - Train Loss: 0.2376172013580799 - Test Loss: 0.25182619839906695\n",
            "Epoch 1, step 300 - Total_idx 46 - Train Loss: 0.2935458612442017 - Test Loss: 0.2790196754038334\n",
            "Epoch 1, step 350 - Total_idx 47 - Train Loss: 0.2836814086139202 - Test Loss: 0.21317145153880118\n",
            "Epoch 1, step 400 - Total_idx 48 - Train Loss: 0.2718138274550438 - Test Loss: 0.2773485280573368\n",
            "Epoch 1, step 450 - Total_idx 49 - Train Loss: 0.2788866564631462 - Test Loss: 0.22762472406029702\n",
            "Epoch 1, step 500 - Total_idx 50 - Train Loss: 0.21252532333135604 - Test Loss: 0.25228559225797653\n",
            "Epoch 1, step 550 - Total_idx 51 - Train Loss: 0.23680788204073905 - Test Loss: 0.27441162019968035\n",
            "Epoch 1, step 600 - Total_idx 52 - Train Loss: 0.27647616133093833 - Test Loss: 0.27340285778045653\n",
            "Epoch 1, step 650 - Total_idx 53 - Train Loss: 0.23365830019116401 - Test Loss: 0.23888890631496906\n",
            "Epoch 1, step 700 - Total_idx 54 - Train Loss: 0.25957584962248803 - Test Loss: 0.3732215240597725\n",
            "Epoch 1, step 750 - Total_idx 55 - Train Loss: 0.2472674471139908 - Test Loss: 0.18082636073231698\n",
            "Epoch 1, step 800 - Total_idx 56 - Train Loss: 0.2449983333796263 - Test Loss: 0.3151970744132996\n",
            "Epoch 1, step 850 - Total_idx 57 - Train Loss: 0.25386665053665636 - Test Loss: 0.2899795562028885\n",
            "Epoch 1, step 900 - Total_idx 58 - Train Loss: 0.23899383455514908 - Test Loss: 0.19794009625911713\n",
            "Epoch 1, step 950 - Total_idx 59 - Train Loss: 0.2668685919046402 - Test Loss: 0.2251648262143135\n",
            "Epoch 1, step 1000 - Total_idx 60 - Train Loss: 0.2658762413263321 - Test Loss: 0.2716413900256157\n",
            "Epoch 1, step 1050 - Total_idx 61 - Train Loss: 0.2526232386380434 - Test Loss: 0.2866992637515068\n",
            "Epoch 1, step 1100 - Total_idx 62 - Train Loss: 0.23563214585185052 - Test Loss: 0.32400233298540115\n",
            "Epoch 1, step 1150 - Total_idx 63 - Train Loss: 0.2754362475126982 - Test Loss: 0.20305894166231156\n",
            "Epoch 1, step 1200 - Total_idx 64 - Train Loss: 0.254165081679821 - Test Loss: 0.26579668670892714\n",
            "Epoch 1, step 1250 - Total_idx 65 - Train Loss: 0.24956973850727082 - Test Loss: 0.23876821659505368\n",
            "Epoch 1, step 1300 - Total_idx 66 - Train Loss: 0.24030449002981186 - Test Loss: 0.214096862077713\n",
            "Epoch 1, step 1350 - Total_idx 67 - Train Loss: 0.262507111877203 - Test Loss: 0.3405898533761501\n",
            "Epoch 1, step 1400 - Total_idx 68 - Train Loss: 0.2319897809624672 - Test Loss: 0.13015188425779342\n",
            "Epoch 1, step 1450 - Total_idx 69 - Train Loss: 0.2524608615785837 - Test Loss: 0.294445451721549\n",
            "Epoch 1, step 1500 - Total_idx 70 - Train Loss: 0.26569949485361577 - Test Loss: 0.23709958791732788\n",
            "Epoch 1, step 1550 - Total_idx 71 - Train Loss: 0.2188520247489214 - Test Loss: 0.21552986279129982\n",
            "Epoch 1, step 1600 - Total_idx 72 - Train Loss: 0.22299087904393672 - Test Loss: 0.2848923794925213\n",
            "Epoch 1, step 1650 - Total_idx 73 - Train Loss: 0.27026876896619795 - Test Loss: 0.31993609070777895\n",
            "Epoch 1, step 1700 - Total_idx 74 - Train Loss: 0.21998304061591625 - Test Loss: 0.35785508900880814\n",
            "Epoch 1, step 1750 - Total_idx 75 - Train Loss: 0.22292995423078538 - Test Loss: 0.264127191901207\n",
            "Epoch 1, step 1800 - Total_idx 76 - Train Loss: 0.24835436455905438 - Test Loss: 0.38447449803352357\n",
            "Epoch 1, step 1850 - Total_idx 77 - Train Loss: 0.2352486838400364 - Test Loss: 0.24350911118090152\n",
            "Epoch 1, step 1900 - Total_idx 78 - Train Loss: 0.2643607347458601 - Test Loss: 0.19414610415697098\n",
            "Epoch 1, step 1950 - Total_idx 79 - Train Loss: 0.24578509755432607 - Test Loss: 0.3649931810796261\n",
            "\t* ===================================== *\n",
            "\t* Epoch 1 :\n",
            "\t* \t Average Train Loss: 0.25169824312627315\n",
            "\t* \t Average Test Loss: 0.2666529608145356\n",
            "Epoch 2, step 0 - Total_idx 80 - Train Loss: 0.46180495619773865 - Test Loss: 0.2815944295376539\n",
            "Epoch 2, step 50 - Total_idx 81 - Train Loss: 0.24257391557097435 - Test Loss: 0.18830844275653363\n",
            "Epoch 2, step 100 - Total_idx 82 - Train Loss: 0.2051434303075075 - Test Loss: 0.3118808150291443\n",
            "Epoch 2, step 150 - Total_idx 83 - Train Loss: 0.2800966474413872 - Test Loss: 0.28988751247525213\n",
            "Epoch 2, step 200 - Total_idx 84 - Train Loss: 0.23687975615262985 - Test Loss: 0.22417901679873467\n",
            "Epoch 2, step 250 - Total_idx 85 - Train Loss: 0.18985981367528437 - Test Loss: 0.29951150342822075\n",
            "Epoch 2, step 300 - Total_idx 86 - Train Loss: 0.2310205864906311 - Test Loss: 0.24402054622769356\n",
            "Epoch 2, step 350 - Total_idx 87 - Train Loss: 0.2317858063429594 - Test Loss: 0.2857890255749226\n",
            "Epoch 2, step 400 - Total_idx 88 - Train Loss: 0.22358343534171582 - Test Loss: 0.21043831408023833\n",
            "Epoch 2, step 450 - Total_idx 89 - Train Loss: 0.22135160200297832 - Test Loss: 0.21494897939264773\n",
            "Epoch 2, step 500 - Total_idx 90 - Train Loss: 0.21145377829670906 - Test Loss: 0.24234798811376096\n",
            "Epoch 2, step 550 - Total_idx 91 - Train Loss: 0.24262015476822854 - Test Loss: 0.19883287996053695\n",
            "Epoch 2, step 600 - Total_idx 92 - Train Loss: 0.2139867778122425 - Test Loss: 0.30524832718074324\n",
            "Epoch 2, step 650 - Total_idx 93 - Train Loss: 0.21076514653861522 - Test Loss: 0.2564547207206488\n",
            "Epoch 2, step 700 - Total_idx 94 - Train Loss: 0.21878452353179456 - Test Loss: 0.23488259874284267\n",
            "Epoch 2, step 750 - Total_idx 95 - Train Loss: 0.23358826875686645 - Test Loss: 0.19901260510087013\n",
            "Epoch 2, step 800 - Total_idx 96 - Train Loss: 0.2263470770418644 - Test Loss: 0.2603291656821966\n",
            "Epoch 2, step 850 - Total_idx 97 - Train Loss: 0.2222032067924738 - Test Loss: 0.16905702650547028\n",
            "Epoch 2, step 900 - Total_idx 98 - Train Loss: 0.21521315179765224 - Test Loss: 0.24960457608103753\n",
            "Epoch 2, step 950 - Total_idx 99 - Train Loss: 0.2398734475672245 - Test Loss: 0.19896368607878684\n",
            "Epoch 2, step 1000 - Total_idx 100 - Train Loss: 0.2512363335490227 - Test Loss: 0.2369994271546602\n",
            "Epoch 2, step 1050 - Total_idx 101 - Train Loss: 0.231660832837224 - Test Loss: 0.24120290875434874\n",
            "Epoch 2, step 1100 - Total_idx 102 - Train Loss: 0.2700029067322612 - Test Loss: 0.2591483727097511\n",
            "Epoch 2, step 1150 - Total_idx 103 - Train Loss: 0.2462953260540962 - Test Loss: 0.235602343454957\n",
            "Epoch 2, step 1200 - Total_idx 104 - Train Loss: 0.19979042932391167 - Test Loss: 0.3442376770079136\n",
            "Epoch 2, step 1250 - Total_idx 105 - Train Loss: 0.207611852735281 - Test Loss: 0.1616335317492485\n",
            "Epoch 2, step 1300 - Total_idx 106 - Train Loss: 0.2160559606552124 - Test Loss: 0.2833478346467018\n",
            "Epoch 2, step 1350 - Total_idx 107 - Train Loss: 0.23411927811801433 - Test Loss: 0.2569348156452179\n",
            "Epoch 2, step 1400 - Total_idx 108 - Train Loss: 0.1995439875870943 - Test Loss: 0.18478463552892208\n",
            "Epoch 2, step 1450 - Total_idx 109 - Train Loss: 0.20120490446686745 - Test Loss: 0.20437110140919684\n",
            "Epoch 2, step 1500 - Total_idx 110 - Train Loss: 0.18740294218063355 - Test Loss: 0.2583741620182991\n",
            "Epoch 2, step 1550 - Total_idx 111 - Train Loss: 0.2100535572692752 - Test Loss: 0.27411525771021844\n",
            "Epoch 2, step 1600 - Total_idx 112 - Train Loss: 0.26221776820719245 - Test Loss: 0.2944413498044014\n",
            "Epoch 2, step 1650 - Total_idx 113 - Train Loss: 0.19735409803688525 - Test Loss: 0.18311012908816338\n",
            "Epoch 2, step 1700 - Total_idx 114 - Train Loss: 0.23774520970880986 - Test Loss: 0.24352846145629883\n",
            "Epoch 2, step 1750 - Total_idx 115 - Train Loss: 0.2123898483812809 - Test Loss: 0.2323933631181717\n",
            "Epoch 2, step 1800 - Total_idx 116 - Train Loss: 0.21357660435140133 - Test Loss: 0.17496490478515625\n",
            "Epoch 2, step 1850 - Total_idx 117 - Train Loss: 0.22208262138068677 - Test Loss: 0.34488086104393006\n",
            "Epoch 2, step 1900 - Total_idx 118 - Train Loss: 0.2301426839083433 - Test Loss: 0.10177720375359059\n",
            "Epoch 2, step 1950 - Total_idx 119 - Train Loss: 0.19144838590174915 - Test Loss: 0.28716708421707154\n",
            "\t* ===================================== *\n",
            "\t* Epoch 2 :\n",
            "\t* \t Average Train Loss: 0.2243371469741687\n",
            "\t* \t Average Test Loss: 0.24170768961310388\n",
            "Epoch 3, step 0 - Total_idx 120 - Train Loss: 0.2229350358247757 - Test Loss: 0.2285999830812216\n",
            "Epoch 3, step 50 - Total_idx 121 - Train Loss: 0.2386715891957283 - Test Loss: 0.1923133585602045\n",
            "Epoch 3, step 100 - Total_idx 122 - Train Loss: 0.2136695495247841 - Test Loss: 0.2816964969038963\n",
            "Epoch 3, step 150 - Total_idx 123 - Train Loss: 0.2409502401202917 - Test Loss: 0.30073694735765455\n",
            "Epoch 3, step 200 - Total_idx 124 - Train Loss: 0.23859294172376394 - Test Loss: 0.33925592750310896\n",
            "Epoch 3, step 250 - Total_idx 125 - Train Loss: 0.18907309956848622 - Test Loss: 0.2450556993484497\n",
            "Epoch 3, step 300 - Total_idx 126 - Train Loss: 0.1956841067224741 - Test Loss: 0.3661274842917919\n",
            "Epoch 3, step 350 - Total_idx 127 - Train Loss: 0.21454391844570636 - Test Loss: 0.21387603804469107\n",
            "Epoch 3, step 400 - Total_idx 128 - Train Loss: 0.19799581103026867 - Test Loss: 0.17974280714988708\n",
            "Epoch 3, step 450 - Total_idx 129 - Train Loss: 0.19570271607488393 - Test Loss: 0.3361139163374901\n",
            "Epoch 3, step 500 - Total_idx 130 - Train Loss: 0.18392227247357368 - Test Loss: 0.24770753160119058\n",
            "Epoch 3, step 550 - Total_idx 131 - Train Loss: 0.2055022295936942 - Test Loss: 0.17307187132537366\n",
            "Epoch 3, step 600 - Total_idx 132 - Train Loss: 0.2064440168067813 - Test Loss: 0.2831416867673397\n",
            "Epoch 3, step 650 - Total_idx 133 - Train Loss: 0.21971043691039085 - Test Loss: 0.26997149363160133\n",
            "Epoch 3, step 700 - Total_idx 134 - Train Loss: 0.23925309136509895 - Test Loss: 0.20303515084087848\n",
            "Epoch 3, step 750 - Total_idx 135 - Train Loss: 0.22676483862102031 - Test Loss: 0.2758517898619175\n",
            "Epoch 3, step 800 - Total_idx 136 - Train Loss: 0.2012564916163683 - Test Loss: 0.22894695922732353\n",
            "Epoch 3, step 850 - Total_idx 137 - Train Loss: 0.21986357051879168 - Test Loss: 0.2548836041241884\n",
            "Epoch 3, step 900 - Total_idx 138 - Train Loss: 0.17453755035996438 - Test Loss: 0.19242046400904655\n",
            "Epoch 3, step 950 - Total_idx 139 - Train Loss: 0.2224708618968725 - Test Loss: 0.20688498374074699\n",
            "Epoch 3, step 1000 - Total_idx 140 - Train Loss: 0.20142665717750788 - Test Loss: 0.22126236259937287\n",
            "Epoch 3, step 1050 - Total_idx 141 - Train Loss: 0.19027815390378236 - Test Loss: 0.1799994608387351\n",
            "Epoch 3, step 1100 - Total_idx 142 - Train Loss: 0.20868520759046078 - Test Loss: 0.3113445330411196\n",
            "Epoch 3, step 1150 - Total_idx 143 - Train Loss: 0.2069592472165823 - Test Loss: 0.24322035871446132\n",
            "Epoch 3, step 1200 - Total_idx 144 - Train Loss: 0.1764176320284605 - Test Loss: 0.22744463868439196\n",
            "Epoch 3, step 1250 - Total_idx 145 - Train Loss: 0.18039487902075052 - Test Loss: 0.1963014978915453\n",
            "Epoch 3, step 1300 - Total_idx 146 - Train Loss: 0.17700790420174597 - Test Loss: 0.24259640984237194\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e5317dc6bd86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-86c3dd4f95d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, nb_epoch)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;31m# Delete cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0;31m# Set batch's data to gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \"\"\"\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = distilBERT()\n",
        "model.train(dataset, nb_epoch=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For assessment tools, we can discuss them directly. \n",
        "Julien "
      ],
      "metadata": {
        "id": "e3fzGbwysA2m"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJxQZEYIJjA3/B6XDqzUY7",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}